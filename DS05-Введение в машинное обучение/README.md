# Введение в машинное обучение

>Машинное обучение с учителем

### Task 1
Колонку **ID** сделаем индексом таблицы

### Task 2
Признаки сохраним в переменную `X`. Целевую переменную **'TARGET'**, которую мы будем предсказывать, в переменную `Y` \
Выведем размерность этих таблиц

### Task 3
Признаки содержат в себе как количественные, так и категориальные переменные \
Категориальные переменные сохрани в переменную `X_cat`, а количественные в `X_num` \
Выведем число категориальных и количественные переменных

### Task 4
Предобработка количественныех переменных. Проведём стандартизацию для признаков `X_num` \
Выведем среднее и стандартное отклонение по датасету для каждого признака после стандартизации

### Task 5
Трансформируем категориальными признаками с помощью *One-Hot Encoding*, т.к. алгоритм машинного обучения плохо работает с категориальными признаками \
Удали признак *CLNT_JOB_POSITION*, т.к. у него слишком много значений \
Выведем количество колонок таблицы `X_cat_ohe`

### Task 6
Два предобработанных датасета объединим в один. Результат запиши в переменную `X_ready`. Датасет сохраняем в папку *datasets/data_prepared.csv* \
Получившийся датасет делим на **train** и **test**. Процент тестовой выборки 20% \
Выведем размерность обучающей выборки `X_train`

### Task 7
Обучаем логистическую регрессию \
Посчитаем точность модели `accuracy` на тестовой выборке, сравним предсказанные и реальные значения

### Task 8
Посчитаем процент клиентов в тестовой выборке, у которых индикатор оттока равен 0, и сравним с `accuracy`

### Task 9
Построим график,  топ-10 самых важных факторов по мнению модели в абсолютном значении с помощью функции [feature_importanse](../code-samples/feature_importanse.py)
